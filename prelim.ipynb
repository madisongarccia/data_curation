{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.indeed.com/jobs?q=Data+Science&l=Los+Angeles%2C+CA&vjk=74cadd03cb0dd918'\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "MaxRetryError",
     "evalue": "HTTPConnectionPool(host='localhost', port=51799): Max retries exceeded with url: /session/32b9b482c2e2a9341d0974593036238d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1369b6f10>: Failed to establish a new connection: [Errno 61] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/site-packages/urllib3/util/connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetaddrinfo returns an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/site-packages/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    716\u001b[0m     conn,\n\u001b[1;32m    717\u001b[0m     method,\n\u001b[1;32m    718\u001b[0m     url,\n\u001b[1;32m    719\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    720\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    721\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    722\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    723\u001b[0m )\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/site-packages/urllib3/connectionpool.py:416\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m         conn\u001b[38;5;241m.\u001b[39mrequest(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhttplib_request_kw)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/site-packages/urllib3/connection.py:244\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    243\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28msuper\u001b[39m(HTTPConnection, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders)\n",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/http/client.py:1294\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1294\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/http/client.py:1340\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1339\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders(body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/http/client.py:1289\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/http/client.py:1048\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1048\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1051\u001b[0m \n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/http/client.py:986\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 986\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/site-packages/urllib3/connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x1369b6f10>: Failed to establish a new connection: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 130\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page_no \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m    129\u001b[0m     url \u001b[38;5;241m=\u001b[39m paginaton_url\u001b[38;5;241m.\u001b[39mformat(job_search_keyword[\u001b[38;5;241m0\u001b[39m], location_search_keyword, page_no)\n\u001b[0;32m--> 130\u001b[0m     page_dom \u001b[38;5;241m=\u001b[39m get_dom(url)\n\u001b[1;32m    131\u001b[0m     jobs \u001b[38;5;241m=\u001b[39m page_dom\u001b[38;5;241m.\u001b[39mxpath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m//div[@class=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob_seen_beacon\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    132\u001b[0m     all_jobs \u001b[38;5;241m=\u001b[39m all_jobs\u001b[38;5;241m.\u001b[39mappend(jobs)\n",
      "Cell \u001b[0;32mIn[20], line 22\u001b[0m, in \u001b[0;36mget_dom\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dom\u001b[39m(url):\n\u001b[0;32m---> 22\u001b[0m    driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m     23\u001b[0m    page_content \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mpage_source\n\u001b[1;32m     24\u001b[0m    product_soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(page_content, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:357\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mGET, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: url})\n",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:346\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    344\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[0;32m--> 346\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py:300\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    298\u001b[0m data \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mdump_json(params)\n\u001b[1;32m    299\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(command_info[\u001b[38;5;241m0\u001b[39m], url, body\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py:321\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    318\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 321\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m    322\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/site-packages/urllib3/request.py:81\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[1;32m     78\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[1;32m     79\u001b[0m     )\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[1;32m     82\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[1;32m     83\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/site-packages/urllib3/request.py:173\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    170\u001b[0m extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(headers)\n\u001b[1;32m    171\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw)\n",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/site-packages/urllib3/poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    374\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, u\u001b[38;5;241m.\u001b[39mrequest_uri, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    378\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/site-packages/urllib3/connectionpool.py:827\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    825\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    826\u001b[0m     )\n\u001b[0;32m--> 827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    828\u001b[0m         method,\n\u001b[1;32m    829\u001b[0m         url,\n\u001b[1;32m    830\u001b[0m         body,\n\u001b[1;32m    831\u001b[0m         headers,\n\u001b[1;32m    832\u001b[0m         retries,\n\u001b[1;32m    833\u001b[0m         redirect,\n\u001b[1;32m    834\u001b[0m         assert_same_host,\n\u001b[1;32m    835\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    836\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[1;32m    837\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[1;32m    838\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    839\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw\n\u001b[1;32m    841\u001b[0m     )\n\u001b[1;32m    843\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    844\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/site-packages/urllib3/connectionpool.py:827\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    825\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    826\u001b[0m     )\n\u001b[0;32m--> 827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    828\u001b[0m         method,\n\u001b[1;32m    829\u001b[0m         url,\n\u001b[1;32m    830\u001b[0m         body,\n\u001b[1;32m    831\u001b[0m         headers,\n\u001b[1;32m    832\u001b[0m         retries,\n\u001b[1;32m    833\u001b[0m         redirect,\n\u001b[1;32m    834\u001b[0m         assert_same_host,\n\u001b[1;32m    835\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    836\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[1;32m    837\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[1;32m    838\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    839\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw\n\u001b[1;32m    841\u001b[0m     )\n\u001b[1;32m    843\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    844\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/site-packages/urllib3/connectionpool.py:827\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    825\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    826\u001b[0m     )\n\u001b[0;32m--> 827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    828\u001b[0m         method,\n\u001b[1;32m    829\u001b[0m         url,\n\u001b[1;32m    830\u001b[0m         body,\n\u001b[1;32m    831\u001b[0m         headers,\n\u001b[1;32m    832\u001b[0m         retries,\n\u001b[1;32m    833\u001b[0m         redirect,\n\u001b[1;32m    834\u001b[0m         assert_same_host,\n\u001b[1;32m    835\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    836\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[1;32m    837\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[1;32m    838\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    839\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw\n\u001b[1;32m    841\u001b[0m     )\n\u001b[1;32m    843\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    844\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/site-packages/urllib3/connectionpool.py:799\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, (SocketError, HTTPException)):\n\u001b[1;32m    797\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 799\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[1;32m    800\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39me, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    801\u001b[0m )\n\u001b[1;32m    802\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Keep track of the error for the retry warning.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/stat386/lib/python3.11/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    581\u001b[0m new_retry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew(\n\u001b[1;32m    582\u001b[0m     total\u001b[38;5;241m=\u001b[39mtotal,\n\u001b[1;32m    583\u001b[0m     connect\u001b[38;5;241m=\u001b[39mconnect,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m    589\u001b[0m )\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_retry\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=51799): Max retries exceeded with url: /session/32b9b482c2e2a9341d0974593036238d/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1369b6f10>: Failed to establish a new connection: [Errno 61] Connection refused'))"
     ]
    }
   ],
   "source": [
    "# website example\n",
    "# import necessary modules\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree as et\n",
    "from csv import writer\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "# define job and location search keywords\n",
    "job_search_keyword = [' Data+Scientist']\n",
    "location_search_keyword = 'Los+Angeles'\n",
    "\n",
    "# define base and pagination URLs\n",
    "base_url = 'https://www.indeed.com'\n",
    "paginaton_url = \"https://www.indeed.com/jobs?q={}&l={}&radius=35&start={}\"\n",
    "\n",
    "#url = 'https://www.indeed.com/jobs?q=Data+Science&l=Los+Angeles%2C+CA&vjk=74cadd03cb0dd918'\n",
    "#driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "#driver.get(url)\n",
    "\n",
    "def get_dom(url):\n",
    "   driver.get(url)\n",
    "   page_content = driver.page_source\n",
    "   product_soup = BeautifulSoup(page_content, 'html.parser')\n",
    "   dom = et.HTML(str(product_soup))\n",
    "   return dom\n",
    "\n",
    "# functions to extract job link\n",
    "def get_job_link(job):\n",
    "   try:\n",
    "       job_link = job.xpath('./descendant::h2/a/@href')[0]\n",
    "   except:\n",
    "       job_link = 'Not available'\n",
    "   return job_link\n",
    "\n",
    "\n",
    "# functions to extract job title\n",
    "def get_job_title(job):\n",
    "   try:\n",
    "       #job_title = job.xpath('./descendant::h2/a/span/text()')[0]\n",
    "       job_title = job.xpath(\".//h2[contains(@class, 'jobTitle')]\").text()\n",
    "   except Exception as e:\n",
    "       job_title = 'Not available'\n",
    "   return job_title\n",
    "\n",
    "\n",
    "# functions to extract the company name\n",
    "def get_company_name(job):\n",
    "   try:\n",
    "       company_name = job.xpath('./descendant::span[@class=\"companyName\"]/text()')[0]\n",
    "   except Exception as e:\n",
    "       company_name = 'Not available'\n",
    "   return company_name\n",
    "\n",
    "\n",
    "# functions to extract the company location\n",
    "def get_company_location(job):\n",
    "   try:\n",
    "       company_location = job.xpath('./descendant::div[@class=\"companyLocation\"]/text()')[0]\n",
    "   except Exception as e:\n",
    "       company_location = 'Not available'\n",
    "   return company_location\n",
    "\n",
    "\n",
    "# functions to extract salary information\n",
    "def get_salary(job):\n",
    "   try:\n",
    "       salary = job.xpath('./descendant::span[@class=\"estimated-salary\"]/span/text()')\n",
    "   except Exception as e:\n",
    "       salary = 'Not available'\n",
    "   if len(salary) == 0:\n",
    "       try:\n",
    "           salary = job.xpath('./descendant::div[@class=\"metadata salary-snippet-container\"]/div/text()')[0]\n",
    "       except Exception as e:\n",
    "           salary = 'Not available'\n",
    "   else:\n",
    "       salary = salary[0]\n",
    "   return salary\n",
    "\n",
    "\n",
    "# functions to extract job type\n",
    "def get_job_type(job):\n",
    "   try:\n",
    "       job_type = job.xpath('./descendant::div[@class=\"metadata\"]/div/text()')[0]\n",
    "   except Exception as e:\n",
    "       job_type = 'Not available'\n",
    "   return job_type\n",
    "\n",
    "\n",
    "# functions to extract job rating\n",
    "def get_rating(job):\n",
    "   try:\n",
    "       rating = job.xpath('./descendant::span[@class=\"ratingNumber\"]/span/text()')[0]\n",
    "   except Exception as e:\n",
    "       rating = 'Not available'\n",
    "   return rating\n",
    "\n",
    "\n",
    "# functions to extract job description\n",
    "def get_job_desc(job):\n",
    "   try:\n",
    "       job_desc = job.xpath('./descendant::div[@class=\"job-snippet\"]/ul/li/text()')\n",
    "   except Exception as e:\n",
    "       job_desc = ['Not available']\n",
    "   if job_desc:\n",
    "       job_desc = \",\".join(job_desc)\n",
    "   else:\n",
    "       job_desc = 'Not available'\n",
    "   return job_desc\n",
    "\n",
    "# Open a CSV file to write the job listings data\n",
    "\n",
    "heading = ['job_link', 'job_title', 'company_name', 'company_location', 'salary', 'job_type', 'rating', 'job_description', 'searched_job', 'searched_location']\n",
    "\n",
    "job_titles = []\n",
    "company_names = []\n",
    "company_ratings = []\n",
    "company_locations = []\n",
    "job_pays = []\n",
    "descriptions = []\n",
    "posted = []\n",
    "salary = []\n",
    "job_type = []\n",
    "rating = []\n",
    "job_desc = []\n",
    "\n",
    "all_jobs = []\n",
    "for page_no in range(0, 100, 10):\n",
    "    url = paginaton_url.format(job_search_keyword[0], location_search_keyword, page_no)\n",
    "    page_dom = get_dom(url)\n",
    "    jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
    "    all_jobs = all_jobs.append(jobs)\n",
    "    \n",
    "for job in all_jobs:\n",
    "    job_link = base_url + get_job_link(job)\n",
    "    job_titles.append(get_job_title(job))\n",
    "    company_names.append(get_company_name(job))\n",
    "    company_locations.append(get_company_location(job))\n",
    "    salary.append(get_salary(job))\n",
    "    job_type.append(get_job_type(job))\n",
    "    rating.append(get_rating(job))\n",
    "    job_desc.append(get_job_desc(job))\n",
    "    #record = [job_link, job_titles, company_names, company_locations, salary, job_type, rating, job_desc, job_search_keyword[0], location_search_keyword]\n",
    "            \n",
    "\n",
    "# Closing the web browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.indeed.com/jobs?q=Data+Science&l=Los+Angeles%2C+CA&vjk=74cadd03cb0dd918'\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = driver.find_element(By.XPATH, \".//div[contains(@id, 'mosaic-jobResults')]\")\n",
    "job_one = container.find_element(By.XPATH, \".//div[contains(@class, 'job_seen_beacon')]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first_title = job_one.find_element(By.XPATH, \".//h2[contains(@class, 'jobTitle')]\").text\n",
    "#first_company = job_one.find_element(By.XPATH, \".//div[contains(@class, 'company_location')]\").text.split(\"\\n\")[0]\n",
    "#first_rating = job_one.find_element(By.XPATH, \".//div[contains(@class, 'company_location')]\").text.split(\"\\n\")[1]\n",
    "#job_one.find_element(By.XPATH, \".//div[contains(@class, 'company_location')]\").text.split(\"\\n\")[2]\n",
    "#n = job_one.find_element(By.XPATH, \".//table[contains(@role, 'presentation')]\").text.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You will work with the chatbots that we are building in order to measure their progress, as well as write and evaluate code.\\nYou can work on your own schedule.\\nEmployer\\nActive 8 days ago'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_one.find_element(By.XPATH, \".//tr[contains(@class, 'underShelfFooter')]\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Active 8 days ag'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#job_one.find_element(By.XPATH, \"//span[contains(@data-testid, 'myJobsStateDate')]\").text.split(\"\\n\")[1]\n",
    "job_one.find_element(By.XPATH, \".//tr[contains(@class, 'underShelfFooter')]\").text.split(\"\\n\")[-1].strip(\"More...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DataAnnotation', '4.5', 'Remote in Los Angeles, CA']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_one.find_element(By.XPATH, \".//div[contains(@class, 'company_location')]\").text.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagination = driver.find_element(By.XPATH, \".//nav[contains(@role, 'navigation')]\")\n",
    "last_page = pagination.find_elements(By.XPATH, \".//a[contains(@data-testid, 'pagination-page-5')]\")[-1].text\n",
    "pagination.find_elements(By.XPATH, \".//li[contains(@class, 'css227srf')]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"e9c39064aae45431d0600ec7323f0d5d\", element=\"f.52729B4287949DABC7A1660F7578F857.d.0A51D195284787AB0F8227C042E95414.e.496\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e9c39064aae45431d0600ec7323f0d5d\", element=\"f.52729B4287949DABC7A1660F7578F857.d.0A51D195284787AB0F8227C042E95414.e.587\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e9c39064aae45431d0600ec7323f0d5d\", element=\"f.52729B4287949DABC7A1660F7578F857.d.0A51D195284787AB0F8227C042E95414.e.651\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e9c39064aae45431d0600ec7323f0d5d\", element=\"f.52729B4287949DABC7A1660F7578F857.d.0A51D195284787AB0F8227C042E95414.e.727\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e9c39064aae45431d0600ec7323f0d5d\", element=\"f.52729B4287949DABC7A1660F7578F857.d.0A51D195284787AB0F8227C042E95414.e.800\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e9c39064aae45431d0600ec7323f0d5d\", element=\"f.52729B4287949DABC7A1660F7578F857.d.0A51D195284787AB0F8227C042E95414.e.804\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e9c39064aae45431d0600ec7323f0d5d\", element=\"f.52729B4287949DABC7A1660F7578F857.d.0A51D195284787AB0F8227C042E95414.e.881\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e9c39064aae45431d0600ec7323f0d5d\", element=\"f.52729B4287949DABC7A1660F7578F857.d.0A51D195284787AB0F8227C042E95414.e.956\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e9c39064aae45431d0600ec7323f0d5d\", element=\"f.52729B4287949DABC7A1660F7578F857.d.0A51D195284787AB0F8227C042E95414.e.1018\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e9c39064aae45431d0600ec7323f0d5d\", element=\"f.52729B4287949DABC7A1660F7578F857.d.0A51D195284787AB0F8227C042E95414.e.1086\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e9c39064aae45431d0600ec7323f0d5d\", element=\"f.52729B4287949DABC7A1660F7578F857.d.0A51D195284787AB0F8227C042E95414.e.1182\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e9c39064aae45431d0600ec7323f0d5d\", element=\"f.52729B4287949DABC7A1660F7578F857.d.0A51D195284787AB0F8227C042E95414.e.1184\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e9c39064aae45431d0600ec7323f0d5d\", element=\"f.52729B4287949DABC7A1660F7578F857.d.0A51D195284787AB0F8227C042E95414.e.1242\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e9c39064aae45431d0600ec7323f0d5d\", element=\"f.52729B4287949DABC7A1660F7578F857.d.0A51D195284787AB0F8227C042E95414.e.1312\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e9c39064aae45431d0600ec7323f0d5d\", element=\"f.52729B4287949DABC7A1660F7578F857.d.0A51D195284787AB0F8227C042E95414.e.1389\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e9c39064aae45431d0600ec7323f0d5d\", element=\"f.52729B4287949DABC7A1660F7578F857.d.0A51D195284787AB0F8227C042E95414.e.1460\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e9c39064aae45431d0600ec7323f0d5d\", element=\"f.52729B4287949DABC7A1660F7578F857.d.0A51D195284787AB0F8227C042E95414.e.1536\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"e9c39064aae45431d0600ec7323f0d5d\", element=\"f.52729B4287949DABC7A1660F7578F857.d.0A51D195284787AB0F8227C042E95414.e.1538\")>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container.find_elements(By.XPATH, \".//li[contains(@class, 'css-5lfssm')]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "# try getting pay amount\n",
    "string = job_one.find_element(By.XPATH, \".//div[contains(@class, 'salary-snippet-container')]\").text\n",
    "listy = string.split()\n",
    "for element in listy:\n",
    "    if \"$\" in element:\n",
    "        pay = element\n",
    "\n",
    "print(pay.strip(\"$\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles = []\n",
    "company_names = []\n",
    "company_ratings = []\n",
    "company_locations = []\n",
    "job_pays = []\n",
    "descriptions = []\n",
    "posted = []\n",
    "current_page = 1\n",
    "\n",
    "pagination = driver.find_element(By.XPATH, \".//nav[contains(@role, 'navigation')]\")\n",
    "last_page = pagination.find_elements(By.XPATH, \".//a[contains(@data-testid, 'pagination-page-5')]\")[-1].text\n",
    "\n",
    "\n",
    "while current_page <= int(last_page):\n",
    "    container = driver.find_element(By.XPATH, \".//div[contains(@id, 'mosaic-jobResults')]\")\n",
    "    all_jobs = container.find_elements(By.XPATH, \".//div[contains(@class, 'job_seen_beacon')]\")\n",
    "    for job in all_jobs:\n",
    "        title = job.find_element(By.XPATH, \".//h2[contains(@class, 'jobTitle')]\").text\n",
    "        try:\n",
    "            location = job.find_element(By.XPATH, \".//div[contains(@class, 'company_location')]\").text.split(\"\\n\")[1]\n",
    "        except:\n",
    "            location = job.find_element(By.XPATH, \".//div[contains(@class, 'company_location')]\").text.split(\"\\n\")[0]\n",
    "        company = job.find_element(By.XPATH, \".//div[contains(@class, 'company_location')]\").text.split(\"\\n\")[0]\n",
    "        when_posted= job.find_element(By.XPATH, \".//tr[contains(@class, 'underShelfFooter')]\").text.split(\"\\n\")[-1].strip(\"More...\")\n",
    "        description = job.find_element(By.XPATH, \".//tr[contains(@class, 'underShelfFooter')]\").text.split(\"\\n\")[0]\n",
    "        try:\n",
    "            string = job.find_element(By.XPATH, \".//div[contains(@class, 'salary-snippet-container')]\").text\n",
    "            listy = string.split()\n",
    "            for element in listy:\n",
    "                if \"$\" in element:\n",
    "                    pay = element\n",
    "        except:\n",
    "            pay = \"NaN\"\n",
    "        job_titles.append(title)\n",
    "        job_pays.append(pay)\n",
    "        company_names.append(company)\n",
    "        descriptions.append(description)\n",
    "        company_locations.append(location)\n",
    "        posted.append(when_posted)\n",
    "    current_page += 1\n",
    "    try:\n",
    "        next = driver.find_element(By.XPATH, \".//a[contains(@data-testid, 'pagination-page-next')]\")\n",
    "        next.click()\n",
    "    except NoSuchElementException:\n",
    "        #next = False\n",
    "        print('No more pages')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goes to 5 pages\n",
    "\"\"\"\n",
    "job_titles = []\n",
    "company_names = []\n",
    "company_ratings = []\n",
    "company_locations = []\n",
    "job_pays = []\n",
    "descriptions = []\n",
    "posted = []\n",
    "current_page = 1\n",
    "\n",
    "pagination = driver.find_element(By.XPATH, \".//nav[contains(@role, 'navigation')]\")\n",
    "last_page = pagination.find_elements(By.XPATH, \".//a[contains(@data-testid, 'pagination-page-5')]\")[-1].text\n",
    "\n",
    "while current_page <= int(last_page):\n",
    "    container = driver.find_element(By.XPATH, \".//div[contains(@id, 'mosaic-jobResults')]\")\n",
    "    all_jobs = container.find_elements(By.XPATH, \".//div[contains(@class, 'job_seen_beacon')]\")\n",
    "    for job in all_jobs:\n",
    "        title = job.find_element(By.XPATH, \".//h2[contains(@class, 'jobTitle')]\").text\n",
    "        try:\n",
    "            location = job.find_element(By.XPATH, \".//div[contains(@class, 'company_location')]\").text.split(\"\\n\")[1]\n",
    "        except:\n",
    "            location = job.find_element(By.XPATH, \".//div[contains(@class, 'company_location')]\").text.split(\"\\n\")[0]\n",
    "        company = job.find_element(By.XPATH, \".//div[contains(@class, 'company_location')]\").text.split(\"\\n\")[0]\n",
    "        when_posted= job.find_element(By.XPATH, \".//tr[contains(@class, 'underShelfFooter')]\").text.split(\"\\n\")[-1].strip(\"More...\")\n",
    "        description = job.find_element(By.XPATH, \".//tr[contains(@class, 'underShelfFooter')]\").text.split(\"\\n\")[0]\n",
    "        job_titles.append(title)\n",
    "        job_pays.append(pay)\n",
    "        company_names.append(company)\n",
    "        descriptions.append(description)\n",
    "        company_locations.append(location)\n",
    "        posted.append(when_posted)\n",
    "    current_page += 1\n",
    "    try:\n",
    "        next = driver.find_element(By.XPATH, \".//a[contains(@data-testid, 'pagination-page-next')]\")\n",
    "        next.click()\n",
    "    except:\n",
    "        pass\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\njob_titles = []\\ncompany_names = []\\ncompany_ratings = []\\ncompany_locations = []\\njob_pays = []\\ndescriptions = []\\nposted = []\\ncurrent_page = 1\\n\\npagination = driver.find_element(By.XPATH, \".//nav[contains(@role, \\'navigation\\')]\")\\nlast_page = pagination.find_elements(By.XPATH, \".//a[contains(@data-testid, \\'pagination-page-5\\')]\")[-1].text\\n\\nwhile current_page <= int(last_page):\\n    container = driver.find_element(By.XPATH, \".//div[contains(@id, \\'mosaic-jobResults\\')]\")\\n    all_jobs = container.find_elements(By.XPATH, \".//div[contains(@class, \\'job_seen_beacon\\')]\")\\n    for job in all_jobs:\\n        title = job.find_element(By.XPATH, \".//h2[contains(@class, \\'jobTitle\\')]\").text\\n        company = job.find_element(By.XPATH, \".//div[contains(@class, \\'company_location\\')]\").text.split(\"\\n\")[0]\\n        location = job.find_element(By.XPATH, \".//div[contains(@class, \\'company_location\\')]\").text.split(\"\\n\")[1]\\n        when_posted= job.find_element(By.XPATH, \"//span[contains(@data-testid, \\'myJobsStateDate\\')]\").text.split(\"\\n\")[1]\\n        description = job.find_element(By.XPATH, \"//div[contains(@class, \\'css-9446fg\\')]\").text\\n        job_titles.append(title)\\n        company_names.append(company)\\n        descriptions.append(description)\\n        company_locations.append(location)\\n        posted.append(when_posted)\\n    current_page += 1\\n    try:\\n        next = driver.find_element(By.XPATH, \".//a[contains(@data-testid, \\'pagination-page-next\\')]\")\\n        next.click()\\n    except:\\n        pass\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "job_titles = []\n",
    "company_names = []\n",
    "company_ratings = []\n",
    "company_locations = []\n",
    "job_pays = []\n",
    "descriptions = []\n",
    "posted = []\n",
    "current_page = 1\n",
    "\n",
    "pagination = driver.find_element(By.XPATH, \".//nav[contains(@role, 'navigation')]\")\n",
    "last_page = pagination.find_elements(By.XPATH, \".//a[contains(@data-testid, 'pagination-page-5')]\")[-1].text\n",
    "\n",
    "while current_page <= int(last_page):\n",
    "    container = driver.find_element(By.XPATH, \".//div[contains(@id, 'mosaic-jobResults')]\")\n",
    "    all_jobs = container.find_elements(By.XPATH, \".//div[contains(@class, 'job_seen_beacon')]\")\n",
    "    for job in all_jobs:\n",
    "        title = job.find_element(By.XPATH, \".//h2[contains(@class, 'jobTitle')]\").text\n",
    "        company = job.find_element(By.XPATH, \".//div[contains(@class, 'company_location')]\").text.split(\"\\n\")[0]\n",
    "        location = job.find_element(By.XPATH, \".//div[contains(@class, 'company_location')]\").text.split(\"\\n\")[1]\n",
    "        when_posted= job.find_element(By.XPATH, \"//span[contains(@data-testid, 'myJobsStateDate')]\").text.split(\"\\n\")[1]\n",
    "        description = job.find_element(By.XPATH, \"//div[contains(@class, 'css-9446fg')]\").text\n",
    "        job_titles.append(title)\n",
    "        company_names.append(company)\n",
    "        descriptions.append(description)\n",
    "        company_locations.append(location)\n",
    "        posted.append(when_posted)\n",
    "    current_page += 1\n",
    "    try:\n",
    "        next = driver.find_element(By.XPATH, \".//a[contains(@data-testid, 'pagination-page-next')]\")\n",
    "        next.click()\n",
    "    except:\n",
    "        pass\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate into city and state\n",
    "cities = []\n",
    "states = []\n",
    "# Parse location data\n",
    "for location in company_locations:\n",
    "    parts = location.split(', ')\n",
    "    if len(parts) > 1:\n",
    "        city = parts[0]\n",
    "        state = parts[1] if len(parts) > 1 else None\n",
    "        cities.append(city)\n",
    "        states.append(state[:2])\n",
    "    else:\n",
    "        cities.append(np.nan)\n",
    "        states.append(np.nan)\n",
    "#make salaries integers\n",
    "salaries = []\n",
    "for salary in job_pays:\n",
    "    if salary == \"NaN\" or salary == \"\":\n",
    "        salaries.append(np.nan)\n",
    "    else:\n",
    "        salaries.append(int(salary.replace('$', '').replace(',', '')))\n",
    "job_info = pd.DataFrame({\"Title\": job_titles,\n",
    "              \"Company\": company_names,\n",
    "              \"Location\": company_locations,\n",
    "              \"Job Description\": descriptions,\n",
    "              \"When Posted\": posted,\n",
    "              \"Pay\": salaries,\n",
    "              \"City\":cities,\n",
    "               \"State\": states})\n",
    "#try getting hybrid or remote status\n",
    "def is_hybrid_remote(location):\n",
    "    return 'Hybrid' in location or 'Remote' in location\n",
    "\n",
    "# Apply the function to create a new column 'Is_Hybrid_Remote'\n",
    "job_info['Is_Hybrid_Remote'] = job_info['Location'].apply(is_hybrid_remote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#job_info.to_csv(\"more_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat386",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
